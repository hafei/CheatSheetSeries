"""
OWASP å®‰å…¨è§„åˆ™æå–æµæ°´çº¿

ä¸»æµæ°´çº¿æ¨¡å—ï¼Œæ•´åˆåˆ†ç‰‡ã€LLMè°ƒç”¨å’Œç»“æœå¤„ç†
"""
import os
import json
import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Optional, Dict, Any, Generator
from dataclasses import dataclass, asdict
import re

from .chunker import MarkdownChunker, MarkdownSection, get_code_language_display
from .prompts import (
    get_system_prompt, 
    get_extraction_prompt, 
    detect_vulnerability_type,
    validate_rule
)
from .llm_client import create_llm_client, BaseLLMClient, LLMResponse


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class PipelineStats:
    """æµæ°´çº¿ç»Ÿè®¡"""
    total_files: int = 0
    processed_files: int = 0
    total_chunks: int = 0
    processed_chunks: int = 0
    total_rules: int = 0
    failed_chunks: int = 0
    total_tokens: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    @property
    def duration_seconds(self) -> float:
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0


class OWASPExtractionPipeline:
    """OWASPå®‰å…¨è§„åˆ™æå–æµæ°´çº¿
    
    ä¸»è¦åŠŸèƒ½:
    1. æ‰«æMarkdownæ–‡ä»¶
    2. æ™ºèƒ½åˆ†ç‰‡
    3. è°ƒç”¨LLMæå–è§„åˆ™
    4. éªŒè¯å’Œå­˜å‚¨ç»“æœ
    """
    
    def __init__(
        self,
        # LLMé…ç½®
        llm_provider: str = "openai",
        llm_model: str = "gpt-4o-mini",
        llm_api_key: Optional[str] = None,
        llm_base_url: Optional[str] = None,
        llm_temperature: float = 0.1,
        llm_max_tokens: int = 4096,
        # åˆ†ç‰‡é…ç½®
        chunk_min_length: int = 100,
        chunk_max_length: int = 8000,
        include_code_required: bool = False,
        # è¿è¡Œé…ç½®
        max_concurrent: int = 3,
        retry_count: int = 3,
        retry_delay: float = 1.0,
        # è¾“å‡ºé…ç½®
        output_dir: str = "./output",
    ):
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        self.llm_client = create_llm_client(
            provider=llm_provider,
            api_key=llm_api_key,
            base_url=llm_base_url,
            model=llm_model,
            temperature=llm_temperature,
            max_tokens=llm_max_tokens
        )
        
        # åˆ›å»ºåˆ†ç‰‡å™¨
        self.chunker = MarkdownChunker(
            min_chunk_length=chunk_min_length,
            max_chunk_length=chunk_max_length,
            include_code_required=include_code_required
        )
        
        # é…ç½®
        self.max_concurrent = max_concurrent
        self.retry_count = retry_count
        self.retry_delay = retry_delay
        self.output_dir = Path(output_dir)
        
        # ç»Ÿè®¡
        self.stats = PipelineStats()
        
        # ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        self._semaphore: Optional[asyncio.Semaphore] = None
    
    def scan_files(self, input_dir: Path, pattern: str = "*.md") -> List[Path]:
        """æ‰«æç›®å½•ä¸‹çš„Markdownæ–‡ä»¶"""
        files = list(input_dir.glob(pattern))
        self.stats.total_files = len(files)
        logger.info(f"æ‰«æåˆ° {len(files)} ä¸ªMarkdownæ–‡ä»¶")
        return sorted(files)
    
    def chunk_file(self, file_path: Path) -> List[MarkdownSection]:
        """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†ç‰‡"""
        try:
            sections = self.chunker.parse_file(file_path)
            logger.info(f"æ–‡ä»¶ {file_path.name} åˆ†ç‰‡: {len(sections)} ä¸ª")
            return sections
        except Exception as e:
            logger.error(f"åˆ†ç‰‡å¤±è´¥ {file_path}: {e}")
            return []
    
    async def extract_rules_from_chunk(
        self, 
        section: MarkdownSection
    ) -> List[Dict[str, Any]]:
        """ä»å•ä¸ªåˆ†ç‰‡æå–å®‰å…¨è§„åˆ™"""
        # æ£€æµ‹æ¼æ´ç±»å‹
        vuln_hint = detect_vulnerability_type(
            section.file_name, 
            section.content
        )
        
        # æ„å»ºprompt
        prompt = get_extraction_prompt(
            file_name=section.file_name,
            doc_title=section.title,
            section_title=section.section_title,
            parent_sections=section.parent_sections,
            content=section.content,
            vuln_hint=vuln_hint
        )
        
        messages = [
            {"role": "system", "content": get_system_prompt()},
            {"role": "user", "content": prompt}
        ]
        
        # é‡è¯•é€»è¾‘
        last_error = None
        for attempt in range(self.retry_count):
            try:
                async with self._semaphore:
                    response = await self.llm_client.chat(messages)
                
                # æ›´æ–°tokenç»Ÿè®¡
                self.stats.total_tokens += response.usage.get("total_tokens", 0)
                
                # è§£æJSONå“åº”
                rules = self._parse_json_response(response.content)
                
                # ä¸ºæ¯ä¸ªè§„åˆ™æ·»åŠ æ¥æºä¿¡æ¯
                for rule in rules:
                    rule["source_file"] = section.file_name
                    rule["section"] = section.section_title
                    
                    # éªŒè¯è§„åˆ™
                    is_valid, issues = validate_rule(rule)
                    if not is_valid:
                        logger.warning(f"è§„åˆ™éªŒè¯é—®é¢˜: {issues}")
                
                return rules
                
            except Exception as e:
                last_error = e
                logger.warning(f"æå–å¤±è´¥ (å°è¯• {attempt + 1}/{self.retry_count}): {e}")
                if attempt < self.retry_count - 1:
                    await asyncio.sleep(self.retry_delay * (attempt + 1))
        
        logger.error(f"æå–æœ€ç»ˆå¤±è´¥: {section.section_title}, é”™è¯¯: {last_error}")
        self.stats.failed_chunks += 1
        return []
    
    def _parse_json_response(self, content: str) -> List[Dict[str, Any]]:
        """è§£æLLMè¿”å›çš„JSON"""
        # æ¸…ç†å“åº”å†…å®¹
        content = content.strip()
        
        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        if content.startswith("```"):
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œ
            first_newline = content.find('\n')
            if first_newline != -1:
                content = content[first_newline + 1:]
            # ç§»é™¤ç»“å°¾çš„```
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
        
        # å°è¯•è§£æJSON
        try:
            result = json.loads(content)
            if isinstance(result, list):
                return result
            elif isinstance(result, dict):
                # å¦‚æœè¿”å›çš„æ˜¯åŒ…å«rulesçš„å¯¹è±¡
                if "rules" in result:
                    return result["rules"]
                return [result]
            return []
        except json.JSONDecodeError as e:
            # å°è¯•ä¿®å¤å¸¸è§JSONé—®é¢˜
            try:
                # å°è¯•æå–JSONæ•°ç»„
                match = re.search(r'\[[\s\S]*\]', content)
                if match:
                    return json.loads(match.group())
            except:
                pass
            
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å†…å®¹: {content[:500]}...")
            return []
    
    async def process_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        all_rules = []
        
        # åˆ†ç‰‡
        sections = self.chunk_file(file_path)
        self.stats.total_chunks += len(sections)
        
        # åˆ›å»ºæå–ä»»åŠ¡
        tasks = []
        for section in sections:
            task = self.extract_rules_from_chunk(section)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"ä»»åŠ¡å¼‚å¸¸: {result}")
                self.stats.failed_chunks += 1
            elif isinstance(result, list):
                all_rules.extend(result)
                self.stats.processed_chunks += 1
        
        self.stats.processed_files += 1
        self.stats.total_rules += len(all_rules)
        
        logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name}, æå–è§„åˆ™: {len(all_rules)} æ¡")
        return all_rules
    
    async def run(
        self, 
        input_dir: str,
        output_file: Optional[str] = None,
        file_pattern: str = "*.md"
    ) -> List[Dict[str, Any]]:
        """
        è¿è¡Œæå–æµæ°´çº¿
        
        Args:
            input_dir: è¾“å…¥ç›®å½•ï¼ˆcheatsheetsæ–‡ä»¶å¤¹ï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            æå–çš„æ‰€æœ‰å®‰å…¨è§„åˆ™
        """
        self.stats = PipelineStats()
        self.stats.start_time = datetime.now()
        self._semaphore = asyncio.Semaphore(self.max_concurrent)
        
        input_path = Path(input_dir)
        if not input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        
        # æ‰«ææ–‡ä»¶
        files = self.scan_files(input_path, file_pattern)
        
        all_rules = []
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for file_path in files:
            try:
                rules = await self.process_file(file_path)
                all_rules.extend(rules)
                
                # ä¸­é—´ä¿å­˜ï¼ˆé˜²æ­¢ä¸­æ–­ä¸¢å¤±ï¼‰
                if output_file and len(all_rules) % 50 == 0:
                    self._save_intermediate(all_rules, output_file)
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        self.stats.end_time = datetime.now()
        
        # æœ€ç»ˆä¿å­˜
        if output_file:
            self._save_results(all_rules, output_file)
        
        # æ‰“å°ç»Ÿè®¡
        self._print_stats()
        
        return all_rules
    
    def run_sync(
        self, 
        input_dir: str,
        output_file: Optional[str] = None,
        file_pattern: str = "*.md"
    ) -> List[Dict[str, Any]]:
        """åŒæ­¥è¿è¡Œæµæ°´çº¿"""
        return asyncio.run(self.run(input_dir, output_file, file_pattern))
    
    def _save_intermediate(self, rules: List[Dict], output_file: str):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        try:
            temp_file = output_file + ".tmp"
            self._save_results(rules, temp_file)
        except Exception as e:
            logger.warning(f"ä¸­é—´ä¿å­˜å¤±è´¥: {e}")
    
    def _save_results(self, rules: List[Dict], output_file: str):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ®æ‰©å±•åé€‰æ‹©æ ¼å¼
        if output_file.endswith('.jsonl'):
            with open(output_path, 'w', encoding='utf-8') as f:
                for rule in rules:
                    f.write(json.dumps(rule, ensure_ascii=False) + '\n')
        else:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(rules, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»“æœå·²ä¿å­˜: {output_path} ({len(rules)} æ¡è§„åˆ™)")
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats
        print("\n" + "=" * 60)
        print("ğŸ“Š æµæ°´çº¿è¿è¡Œç»Ÿè®¡")
        print("=" * 60)
        print(f"  æ–‡ä»¶å¤„ç†: {stats.processed_files}/{stats.total_files}")
        print(f"  åˆ†ç‰‡å¤„ç†: {stats.processed_chunks}/{stats.total_chunks}")
        print(f"  å¤±è´¥åˆ†ç‰‡: {stats.failed_chunks}")
        print(f"  æå–è§„åˆ™: {stats.total_rules} æ¡")
        print(f"  Tokenæ¶ˆè€—: {stats.total_tokens:,}")
        print(f"  è¿è¡Œæ—¶é•¿: {stats.duration_seconds:.1f} ç§’")
        print("=" * 60 + "\n")


def run_extraction(
    input_dir: str,
    output_file: str,
    provider: str = "openai",
    model: str = "gpt-4o-mini",
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    max_concurrent: int = 3,
    **kwargs
) -> List[Dict[str, Any]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè¿è¡ŒOWASPè§„åˆ™æå–
    
    Args:
        input_dir: cheatsheetsç›®å½•è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        provider: LLMæœåŠ¡å•†
        model: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
        base_url: è‡ªå®šä¹‰APIåœ°å€ï¼ˆå¯é€‰ï¼‰
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        
    Returns:
        æå–çš„è§„åˆ™åˆ—è¡¨
    """
    pipeline = OWASPExtractionPipeline(
        llm_provider=provider,
        llm_model=model,
        llm_api_key=api_key,
        llm_base_url=base_url,
        max_concurrent=max_concurrent,
        **kwargs
    )
    
    return pipeline.run_sync(input_dir, output_file)
