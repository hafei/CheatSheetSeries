"""
OWASP å®‰å…¨è§„åˆ™æå–æµæ°´çº¿

ä¸»æµæ°´çº¿æ¨¡å—ï¼Œæ•´åˆåˆ†ç‰‡ã€LLMè°ƒç”¨å’Œç»“æœå¤„ç†
"""
import json
import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
import re

from .chunker import MarkdownChunker, MarkdownSection
from .prompts import (
    get_system_prompt, 
    get_extraction_prompt, 
    detect_vulnerability_type,
    validate_rule
)
from .llm_client import create_llm_client
from .config_loader import load_config, ConfigError
from .models import PipelineConfig


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class PipelineStats:
    """æµæ°´çº¿ç»Ÿè®¡"""
    total_files: int = 0
    processed_files: int = 0
    total_chunks: int = 0
    processed_chunks: int = 0
    total_rules: int = 0
    failed_chunks: int = 0
    total_tokens: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    @property
    def duration_seconds(self) -> float:
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0


class OWASPExtractionPipeline:
    """OWASPå®‰å…¨è§„åˆ™æå–æµæ°´çº¿
    
    ä¸»è¦åŠŸèƒ½:
    1. æ‰«æMarkdownæ–‡ä»¶
    2. æ™ºèƒ½åˆ†ç‰‡
    3. è°ƒç”¨LLMæå–è§„åˆ™
    4. éªŒè¯å’Œå­˜å‚¨ç»“æœ
    """
    
    def __init__(
        self,
        # LLMé…ç½®
        llm_provider: str = "openai",
        llm_model: str = "gpt-4o-mini",
        llm_api_key: Optional[str] = None,
        llm_base_url: Optional[str] = None,
        llm_temperature: float = 0.1,
        llm_max_tokens: int = 4096,
        # åˆ†ç‰‡é…ç½®
        chunk_min_length: int = 100,
        chunk_max_length: int = 8000,
        include_code_required: bool = False,
        # è¿è¡Œé…ç½®
        max_concurrent: int = 3,
        retry_count: int = 3,
        retry_delay: float = 1.0,
        # è¾“å‡ºé…ç½®
        output_dir: str = "./output",
    ):
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        self.llm_client = create_llm_client(
            provider=llm_provider,
            api_key=llm_api_key,
            base_url=llm_base_url,
            model=llm_model,
            temperature=llm_temperature,
            max_tokens=llm_max_tokens
        )
        
        # åˆ›å»ºåˆ†ç‰‡å™¨
        self.chunker = MarkdownChunker(
            min_chunk_length=chunk_min_length,
            max_chunk_length=chunk_max_length,
            include_code_required=include_code_required
        )
        
        # é…ç½®
        self.max_concurrent = max_concurrent
        self.retry_count = retry_count
        self.retry_delay = retry_delay
        self.output_dir = Path(output_dir)
        
        # ç»Ÿè®¡
        self.stats = PipelineStats()
        
        # ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        self._semaphore: Optional[asyncio.Semaphore] = None
    
    @classmethod
    def from_config(cls, cfg: PipelineConfig) -> "OWASPExtractionPipeline":
        """Construct pipeline from a PipelineConfig instance."""
        return cls(
            llm_provider=cfg.llm_provider,
            llm_model=cfg.llm_model,
            llm_api_key=cfg.llm_api_key,
            llm_base_url=cfg.llm_base_url,
            llm_temperature=cfg.llm_temperature,
            llm_max_tokens=cfg.llm_max_tokens,
            chunk_min_length=cfg.chunk_min_length,
            chunk_max_length=cfg.chunk_max_length,
            include_code_required=cfg.include_code_blocks,
            max_concurrent=cfg.max_concurrent,
            retry_count=cfg.retry_count,
            retry_delay=cfg.retry_delay,
            output_dir=cfg.output_dir,
        )

    def scan_files(self, input_dir: Path, pattern: str = "*.md") -> List[Path]:
        """æ‰«æç›®å½•ä¸‹çš„Markdownæ–‡ä»¶"""
        files = list(input_dir.glob(pattern))
        self.stats.total_files = len(files)
        logger.info(f"æ‰«æåˆ° {len(files)} ä¸ªMarkdownæ–‡ä»¶")
        return sorted(files)
    
    def chunk_file(self, file_path: Path) -> List[MarkdownSection]:
        """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†ç‰‡"""
        try:
            sections = self.chunker.parse_file(file_path)
            logger.info(f"æ–‡ä»¶ {file_path.name} åˆ†ç‰‡: {len(sections)} ä¸ª")
            return sections
        except Exception as e:
            logger.error(f"åˆ†ç‰‡å¤±è´¥ {file_path}: {e}")
            return []
    
    async def extract_rules_from_chunk(
        self, 
        section: MarkdownSection
    ) -> List[Dict[str, Any]]:
        """ä»å•ä¸ªåˆ†ç‰‡æå–å®‰å…¨è§„åˆ™"""
        # æ£€æµ‹æ¼æ´ç±»å‹
        vuln_hint = detect_vulnerability_type(
            section.file_name, 
            section.content
        )
        
        # å¦‚æœåˆ†ç‰‡å†…å®¹è¿‡å¤§ï¼Œæˆªæ–­é¦–å°¾ä»¥é¿å…æ¨¡å‹è¾“å‡ºè¢«æˆªæ–­ä¸ºç©º
        MAX_PROMPT_CONTENT = 80000
        content_to_use = section.content
        if len(section.content) > MAX_PROMPT_CONTENT:
            head = section.content[:1500]
            tail = section.content[-1500:]
            content_to_use = head + "\n\n... ï¼ˆä¸­é—´è¢«æˆªæ–­ï¼Œä¸ºäº†ç¡®ä¿æ¨¡å‹èƒ½è¿”å›å®Œæ•´JSONè€Œçœç•¥ï¼‰ ...\n\n" + tail
            logger.info(f"åˆ†ç‰‡å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ file={section.file_name} section={section.section_title}")

        # æ„å»ºprompt
        prompt = get_extraction_prompt(
            file_name=section.file_name,
            doc_title=section.title,
            section_title=section.section_title,
            parent_sections=section.parent_sections,
            content=content_to_use,
            vuln_hint=vuln_hint
        )
        
        messages = [
            {"role": "system", "content": get_system_prompt()},
            {"role": "user", "content": prompt}
        ]
        
        # é‡è¯•é€»è¾‘
        last_error = None
        for attempt in range(self.retry_count):
            try:
                async with self._semaphore:
                    response = await self.llm_client.chat(messages)
                
                # æ›´æ–°tokenç»Ÿè®¡
                self.stats.total_tokens += response.usage.get("total_tokens", 0)
                # å¦‚æœè¿”å›å†…å®¹ä¸ºç©ºï¼Œè®°å½•å¹¶å°† raw_response å†™å…¥ debug æ–‡ä»¶ï¼Œç„¶åè§¦å‘é‡è¯•
                if not (response.content and response.content.strip()):
                    try:
                        log_dir = self.output_dir if hasattr(self, 'output_dir') else Path('./output')
                        log_dir.mkdir(parents=True, exist_ok=True)
                        debug_file = log_dir / 'llm_raw_response_debug.jsonl'
                        debug_obj = {
                            'timestamp': datetime.utcnow().isoformat() + 'Z',
                            'source_file': section.file_name,
                            'section': section.section_title,
                            'raw_response': getattr(response, 'raw_response', None)
                        }
                        with open(debug_file, 'a', encoding='utf-8') as df:
                            df.write(json.dumps(debug_obj, ensure_ascii=False) + '\n')
                    except Exception:
                        pass

                    raise ValueError('Empty LLM response content')

                # è§£æJSONå“åº”ï¼ˆå¹¶è®°å½•åŸå§‹å“åº”ä»¥ä¾¿è°ƒè¯•ï¼‰
                rules = self._parse_json_response(response.content, section, raw_response=getattr(response, 'raw_response', None))
                
                # ä¸ºæ¯ä¸ªè§„åˆ™æ·»åŠ æ¥æºä¿¡æ¯
                for rule in rules:
                    rule["source_file"] = section.file_name
                    rule["section"] = section.section_title
                    
                    # éªŒè¯è§„åˆ™
                    is_valid, issues = validate_rule(rule)
                    if not is_valid:
                        logger.warning(f"è§„åˆ™éªŒè¯é—®é¢˜: {issues}")
                
                return rules
                
            except Exception as e:
                last_error = e
                logger.warning(f"æå–å¤±è´¥ (å°è¯• {attempt + 1}/{self.retry_count}): {e}")
                if attempt < self.retry_count - 1:
                    await asyncio.sleep(self.retry_delay * (attempt + 1))
        
        logger.error(f"æå–æœ€ç»ˆå¤±è´¥: {section.section_title}, é”™è¯¯: {last_error}")
        self.stats.failed_chunks += 1
        return []
    
    def _parse_json_response(self, content: str, section: Optional[MarkdownSection] = None, raw_response: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """è§£æLLMè¿”å›çš„JSON"""
        # æ¸…ç†å“åº”å†…å®¹
        content = content.strip()
        
        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        if content.startswith("```"):
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œ
            first_newline = content.find('\n')
            if first_newline != -1:
                content = content[first_newline + 1:]
            # ç§»é™¤ç»“å°¾çš„```
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
        
        # å°è¯•è§£æJSON
        try:
            result = json.loads(content)
            if isinstance(result, list):
                return result
            elif isinstance(result, dict):
                # å¦‚æœè¿”å›çš„æ˜¯åŒ…å«rulesçš„å¯¹è±¡
                if "rules" in result:
                    return result["rules"]
                return [result]
            return []
        except json.JSONDecodeError as e:
            # å°†åŸå§‹å“åº”è¿½åŠ åˆ°è°ƒè¯•æ—¥å¿—ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
            try:
                log_dir = self.output_dir if hasattr(self, 'output_dir') else Path('./output')
                log_dir.mkdir(parents=True, exist_ok=True)
                # åŸå§‹æ–‡æœ¬æ—¥å¿—ï¼ˆäººç±»å¯è¯»ï¼‰
                log_file = log_dir / 'llm_raw_responses.log'
                with open(log_file, 'a', encoding='utf-8') as lf:
                    header = f"\n--- {datetime.utcnow().isoformat()}Z"
                    if section:
                        header += f" | file={section.file_name} | section={section.section_title}"
                    lf.write(header + '\n')
                    lf.write(content + '\n')

                # ç»“æ„åŒ–åŸå§‹å“åº”ï¼ˆJSONLï¼‰ï¼Œå¦‚æœä¸Šå±‚æä¾›äº† raw_responseï¼Œåˆ™å†™å…¥ä»¥ä¾¿åç»­åˆ†æ
                if raw_response is not None:
                    debug_file = log_dir / 'llm_raw_response_debug.jsonl'
                    # ä¿å­˜åˆ° jsonlï¼Œæ¯è¡Œä¸€ä¸ªå¯¹è±¡ï¼Œæ·»åŠ å…ƒä¿¡æ¯
                    debug_obj = {
                        'timestamp': datetime.utcnow().isoformat() + 'Z',
                        'source_file': section.file_name if section else None,
                        'section': section.section_title if section else None,
                        'raw_response': raw_response
                    }
                    with open(debug_file, 'a', encoding='utf-8') as df:
                        df.write(json.dumps(debug_obj, ensure_ascii=False) + '\n')
            except Exception:
                pass

            # å¦‚æœ response çš„ raw_response æœ‰ç»“æ„ä¿¡æ¯ï¼Œä¹Ÿå°†å…¶å­˜ä¸º jsonl ä»¥ä¾¿åˆ†æï¼ˆé¿å…è¦†ç›–ï¼‰
            try:
                # raw_response å¯åœ¨ä¸Šå±‚ llm_client ä¸­é€šè¿‡ response.raw_response ä¼ å…¥
                # ä½†æ­¤å¤„åªæœ‰ content å­—ç¬¦ä¸²å¯ç”¨â€”â€”å°è¯•ä»è°ƒç”¨æ ˆä¸­å¯»æ‰¾ raw_response ä¸ç°å®
                # å› æ­¤ï¼Œä¸Šå±‚åœ¨è°ƒç”¨ _parse_json_response æ—¶åº”ä¼ å…¥åŸå§‹ LLMResponse å¯¹è±¡æˆ–å­—å…¸
                # ä½œä¸ºè¡¥å……ï¼Œè¿™é‡Œä»…å†™å…¥ content å·²è®°å½•ã€‚å¦‚æœä¸Šå±‚ä¼ å…¥äº†é¢å¤–å±æ€§ï¼Œå¯å¦è¡Œè®°å½•ã€‚
                pass
            except Exception:
                pass

            # å°è¯•ä¿®å¤å¸¸è§JSONé—®é¢˜çš„å¤šç§ç­–ç•¥
            # 1) æå–æœ€åƒJSONçš„æ•°ç»„æˆ–å¯¹è±¡
            try:
                match = re.search(r'\[.*\]', content, flags=re.S)
                if match:
                    candidate = match.group()
                    # ç§»é™¤å°¾éšé€—å·
                    candidate = re.sub(r',\s*\]', ']', candidate)
                    return json.loads(candidate)
            except Exception:
                pass

            try:
                match = re.search(r'\{.*\}', content, flags=re.S)
                if match:
                    candidate = match.group()
                    candidate = re.sub(r',\s*\}', '}', candidate)
                    # å°†å•å¼•å·æ›¿æ¢ä¸ºåŒå¼•å·ï¼ˆè°¨æ…ï¼‰
                    candidate2 = candidate.replace("'", '"')
                    try:
                        return json.loads(candidate2)
                    except Exception:
                        return json.loads(candidate)
            except Exception:
                pass

            # 2) æ›¿æ¢å¸¸è§çš„é”™è¯¯ï¼ˆå°¾éšé€—å·ã€å•å¼•å·ï¼‰å¹¶é‡è¯•
            try:
                s = content
                s = re.sub(r',\s*([\]}])', r'\1', s)
                s = s.replace("'", '"')
                return json.loads(s)
            except Exception:
                pass

            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å†…å®¹å‰500å­—: {content[:500]}...")
            return []
    
    async def process_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        all_rules = []
        
        # åˆ†ç‰‡
        sections = self.chunk_file(file_path)
        self.stats.total_chunks += len(sections)
        
        # åˆ›å»ºæå–ä»»åŠ¡
        tasks = []
        for section in sections:
            task = self.extract_rules_from_chunk(section)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"ä»»åŠ¡å¼‚å¸¸: {result}")
                self.stats.failed_chunks += 1
            elif isinstance(result, list):
                all_rules.extend(result)
                self.stats.processed_chunks += 1
        
        self.stats.processed_files += 1
        self.stats.total_rules += len(all_rules)
        
        logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name}, æå–è§„åˆ™: {len(all_rules)} æ¡")
        return all_rules
    
    async def run(
        self, 
        input_dir: str,
        output_file: Optional[str] = None,
        file_pattern: str = "*.md"
    ) -> List[Dict[str, Any]]:
        """
        è¿è¡Œæå–æµæ°´çº¿
        
        Args:
            input_dir: è¾“å…¥ç›®å½•ï¼ˆcheatsheetsæ–‡ä»¶å¤¹ï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            æå–çš„æ‰€æœ‰å®‰å…¨è§„åˆ™
        """
        self.stats = PipelineStats()
        self.stats.start_time = datetime.now()
        self._semaphore = asyncio.Semaphore(self.max_concurrent)
        
        input_path = Path(input_dir)
        if not input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        
        # æ‰«ææ–‡ä»¶
        files = self.scan_files(input_path, file_pattern)
        
        all_rules = []
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for file_path in files:
            try:
                rules = await self.process_file(file_path)
                all_rules.extend(rules)

                # ä¸­é—´ä¿å­˜ï¼ˆé˜²æ­¢ä¸­æ–­ä¸¢å¤±ï¼‰: æ¯å¤„ç†å®Œä¸€ä¸ªæ–‡ä»¶å°±ä¿å­˜ä¸€æ¬¡
                if output_file:
                    self._save_intermediate(all_rules, output_file)
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        self.stats.end_time = datetime.now()
        
        # æœ€ç»ˆä¿å­˜
        if output_file:
            self._save_results(all_rules, output_file)
        
        # æ‰“å°ç»Ÿè®¡
        self._print_stats()
        
        return all_rules
    
    def run_sync(
        self, 
        input_dir: str,
        output_file: Optional[str] = None,
        file_pattern: str = "*.md"
    ) -> List[Dict[str, Any]]:
        """åŒæ­¥è¿è¡Œæµæ°´çº¿"""
        return asyncio.run(self.run(input_dir, output_file, file_pattern))
    
    def _save_intermediate(self, rules: List[Dict], output_file: str):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        try:
            temp_file = output_file + ".tmp"
            self._save_results(rules, temp_file)
        except Exception as e:
            logger.warning(f"ä¸­é—´ä¿å­˜å¤±è´¥: {e}")
    
    def _save_results(self, rules: List[Dict], output_file: str):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ®æ‰©å±•åé€‰æ‹©æ ¼å¼
        if output_file.endswith('.jsonl'):
            with open(output_path, 'w', encoding='utf-8') as f:
                for rule in rules:
                    f.write(json.dumps(rule, ensure_ascii=False) + '\n')
        else:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(rules, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»“æœå·²ä¿å­˜: {output_path} ({len(rules)} æ¡è§„åˆ™)")
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats
        print("\n" + "=" * 60)
        print("ğŸ“Š æµæ°´çº¿è¿è¡Œç»Ÿè®¡")
        print("=" * 60)
        print(f"  æ–‡ä»¶å¤„ç†: {stats.processed_files}/{stats.total_files}")
        print(f"  åˆ†ç‰‡å¤„ç†: {stats.processed_chunks}/{stats.total_chunks}")
        print(f"  å¤±è´¥åˆ†ç‰‡: {stats.failed_chunks}")
        print(f"  æå–è§„åˆ™: {stats.total_rules} æ¡")
        print(f"  Tokenæ¶ˆè€—: {stats.total_tokens:,}")
        print(f"  è¿è¡Œæ—¶é•¿: {stats.duration_seconds:.1f} ç§’")
        print("=" * 60 + "\n")


def run_extraction(
    input_dir: str,
    output_file: str,
    provider: str = "openai",
    model: str = "gpt-4o-mini",
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    max_concurrent: int = 3,
    **kwargs
) -> List[Dict[str, Any]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè¿è¡ŒOWASPè§„åˆ™æå–
    
    Args:
        input_dir: cheatsheetsç›®å½•è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        provider: LLMæœåŠ¡å•†
        model: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
        base_url: è‡ªå®šä¹‰APIåœ°å€ï¼ˆå¯é€‰ï¼‰
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        
    Returns:
        æå–çš„è§„åˆ™åˆ—è¡¨
    """
    # If a config path is provided in kwargs or via env, prefer that configuration
    config_path = kwargs.pop("config_path", None)
    cfg_obj = None
    if config_path:
        try:
            cfg_obj = load_config(config_path)
        except ConfigError as e:
            logger.error(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
            raise

    if cfg_obj is not None:
        pipeline = OWASPExtractionPipeline.from_config(cfg_obj)
    else:
        pipeline = OWASPExtractionPipeline(
            llm_provider=provider,
            llm_model=model,
            llm_api_key=api_key,
            llm_base_url=base_url,
            max_concurrent=max_concurrent,
            **kwargs
        )

    return pipeline.run_sync(input_dir, output_file)
